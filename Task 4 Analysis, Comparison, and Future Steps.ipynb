{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badcec00-a544-4214-b266-d31c6788f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Task 4 — Config =================\n",
    "# Data sources (prefer the clean engineered CSV you created in Task 1)\n",
    "USE_CLEAN_FIRST = True\n",
    "CLEAN_PATH = \"loan_clean_subset.csv\"             # put the file next to this notebook\n",
    "RAW_PATH   = \"accepted_2007_to_2018Q4.csv\"    # if clean not available, we'll preprocess from raw\n",
    "NROWS_FROM_RAW = 200_000                         # set None for full data if you have RAM\n",
    "\n",
    "# Re-train inside this notebook (True) vs only evaluate saved artifacts (False)\n",
    "RETRAIN_DL = True\n",
    "RETRAIN_RL = True\n",
    "\n",
    "# Saved artifact locations (if you want to load pre-trained)\n",
    "DL_MODEL_DIR = \"model_mlp_default_risk\"          # from your Task-2 Cell 9\n",
    "DL_MODEL_PATH = f\"{DL_MODEL_DIR}/.keras\"         # you customized this path in Task 2\n",
    "RL_ART_DIR    = \"offline_rl_cql\"                 # from Task-3 Cell 9 if you saved there\n",
    "RL_MODEL_PATH = f\"{RL_ART_DIR}/cql_discrete_model.d3\"\n",
    "PREPROC_PATH  = f\"{RL_ART_DIR}/preprocess.joblib\"  # sklearn preprocessor saved in Task-3\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c670bd-d09a-4f78-920f-3519eee444c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============== Installs (Jupyter / Colab) =================\n",
    "# In a fresh environment you may need these:\n",
    "# (On Windows Jupyter, ensure you're on Python 3.11 environment, not 3.13.)\n",
    "\n",
    "!pip -q install -U numpy<2.0 pandas scikit-learn matplotlib joblib\n",
    "\n",
    "# Offline RL stack (d3rlpy + gymnasium + torch). If already installed, this cell will no-op.\n",
    "# If you see errors, ensure you're in Python 3.11 and re-run.\n",
    "!pip -q install -U d3rlpy==2.4.0 gymnasium[classic-control]==0.29.1 torch==2.4.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2de1d-b9bb-4f44-a7f1-1c5a6a59ef3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3456ac4a-7073-465e-bf7e-45e2450a2e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.14 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 18:30:03) [MSC v.1929 64 bit (AMD64)]\n",
      "Torch : 2.4.1+cpu\n",
      "Kernel exe: C:\\Users\\kamya\\anaconda3\\envs\\rl-fintech\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch :\", torch.__version__)\n",
    "print(\"Kernel exe:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564055a-919d-4e3f-8bf2-e75dd085e4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538255d1-5920-4a13-a9b3-df8e2a1406c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f2cd4-d409-4972-95cc-78e6bdccaca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891d5a83-9084-4d8a-9152-4af0af52d437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.15.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (4.25.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (4.15.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.14.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.42.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.9)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.32.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.1.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2025.10.5)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.6.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.45.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kamya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb95dbe5-1f18-471f-8c6d-d718605aa1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_np shape: (141612, 88)\n",
      "X_test_np shape : (35404, 88)\n"
     ]
    }
   ],
   "source": [
    "# === Minimal setup before PyTorch MLP ===\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your clean dataset (or point to your CSV)\n",
    "df = pd.read_csv(\"loan_clean_subset.csv\")   # path to your clean data from Task 1\n",
    "assert \"default\" in df.columns, \"Dataset must include a 'default' column.\"\n",
    "\n",
    "X = df.drop(columns=[\"default\"])\n",
    "y = df[\"default\"].astype(int).values\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "num_features = [c for c in X_train.columns if np.issubdtype(X_train[c].dtype, np.number)]\n",
    "cat_features = [c for c in X_train.columns if not np.issubdtype(X_train[c].dtype, np.number)]\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                     (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_features),\n",
    "    (\"cat\", cat_pipe, cat_features)\n",
    "])\n",
    "\n",
    "# Transform to numpy arrays\n",
    "X_train_np = preprocess.fit_transform(X_train)\n",
    "X_test_np  = preprocess.transform(X_test)\n",
    "\n",
    "print(\"X_train_np shape:\", X_train_np.shape)\n",
    "print(\"X_test_np shape :\", X_test_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87080411-bad8-4798-a398-f4ee1d0ce2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | val_auc=0.7434 | val_loss=0.4426\n",
      "Epoch 02 | val_auc=0.7456 | val_loss=0.4402\n",
      "Epoch 03 | val_auc=0.7456 | val_loss=0.4401\n",
      "Epoch 04 | val_auc=0.7462 | val_loss=0.4403\n",
      "Epoch 05 | val_auc=0.7467 | val_loss=0.4391\n",
      "Epoch 06 | val_auc=0.7464 | val_loss=0.4395\n",
      "Epoch 07 | val_auc=0.7464 | val_loss=0.4397\n",
      "Epoch 08 | val_auc=0.7457 | val_loss=0.4398\n",
      "Epoch 09 | val_auc=0.7455 | val_loss=0.4397\n",
      "Epoch 10 | val_auc=0.7460 | val_loss=0.4403\n",
      "Early stopping.\n",
      "DL (PyTorch) — Test ROC-AUC: 0.7464\n",
      "DL (PyTorch) — Test F1     : 0.2182\n",
      "\n",
      "DL Classification report (thr=0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8151    0.9743    0.8876     28199\n",
      "           1     0.5729    0.1348    0.2182      7205\n",
      "\n",
      "    accuracy                         0.8035     35404\n",
      "   macro avg     0.6940    0.5545    0.5529     35404\n",
      "weighted avg     0.7658    0.8035    0.7514     35404\n",
      "\n",
      "DL Confusion matrix:\n",
      "[[27475   724]\n",
      " [ 6234   971]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamya\\AppData\\Local\\Temp\\ipykernel_18920\\2940505826.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# ==== DL model (PyTorch MLP) — Train & Evaluate AUC/F1 ====\n",
    "import numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "INPUT_DIM = X_train_np.shape[1]\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "VAL_SPLIT = 0.15\n",
    "MODEL_DIR = \"model_mlp_default_risk\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"pytorch_mlp.pt\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.ReLU(), nn.Dropout(0.25),\n",
    "            nn.Linear(256, 128),    nn.ReLU(), nn.Dropout(0.25),\n",
    "            nn.Linear(128, 64),     nn.ReLU(), nn.Dropout(0.25),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # logits\n",
    "\n",
    "# Build dataset & split\n",
    "X_t = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_t = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32)\n",
    "dataset = TensorDataset(X_t, y_t)\n",
    "val_size = int(len(dataset)*VAL_SPLIT)\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLP(INPUT_DIM).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_val_auc = -1.0\n",
    "patience, patience_cnt = 5, 0\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "def eval_auc_loss(dl):\n",
    "    model.eval()\n",
    "    all_logits, all_y, total_loss = [], [], 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = bce(logits, yb)\n",
    "            total_loss += loss.item()*len(xb)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_y.append(yb.cpu())\n",
    "    logits = torch.cat(all_logits).numpy().ravel()\n",
    "    y_true = torch.cat(all_y).numpy().ravel().astype(int)\n",
    "    y_proba = 1/(1+np.exp(-logits))\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    return auc, total_loss/len(dl.dataset)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = bce(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    val_auc, val_loss = eval_auc_loss(val_dl)\n",
    "    print(f\"Epoch {epoch:02d} | val_auc={val_auc:.4f} | val_loss={val_loss:.4f}\")\n",
    "    # Early stopping on val AUC\n",
    "    if val_auc > best_val_auc + 1e-4:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Load best and evaluate on TEST\n",
    "best_model = MLP(INPUT_DIM).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_test = best_model(torch.tensor(X_test_np, dtype=torch.float32).to(DEVICE)).cpu().numpy().ravel()\n",
    "y_proba_dl = 1/(1+np.exp(-logits_test))\n",
    "y_pred_dl  = (y_proba_dl >= 0.5).astype(int)\n",
    "\n",
    "auc_dl = roc_auc_score(y_test, y_proba_dl)\n",
    "f1_dl  = f1_score(y_test, y_pred_dl)\n",
    "\n",
    "print(f\"DL (PyTorch) — Test ROC-AUC: {auc_dl:.4f}\")\n",
    "print(f\"DL (PyTorch) — Test F1     : {f1_dl:.4f}\")\n",
    "print(\"\\nDL Classification report (thr=0.5):\")\n",
    "print(classification_report(y_test, y_pred_dl, digits=4))\n",
    "print(\"DL Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c69ed78f-aa9e-40dc-8586-e829e332cae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | val_auc=0.7434 | val_loss=0.4429\n",
      "Epoch 02 | val_auc=0.7462 | val_loss=0.4409\n",
      "Epoch 03 | val_auc=0.7470 | val_loss=0.4388\n",
      "Epoch 04 | val_auc=0.7473 | val_loss=0.4389\n",
      "Epoch 05 | val_auc=0.7476 | val_loss=0.4383\n",
      "Epoch 06 | val_auc=0.7472 | val_loss=0.4383\n",
      "Epoch 07 | val_auc=0.7473 | val_loss=0.4382\n",
      "Epoch 08 | val_auc=0.7470 | val_loss=0.4392\n",
      "Epoch 09 | val_auc=0.7478 | val_loss=0.4389\n",
      "Epoch 10 | val_auc=0.7471 | val_loss=0.4385\n",
      "Epoch 11 | val_auc=0.7471 | val_loss=0.4384\n",
      "Epoch 12 | val_auc=0.7468 | val_loss=0.4385\n",
      "Epoch 13 | val_auc=0.7458 | val_loss=0.4415\n",
      "Epoch 14 | val_auc=0.7454 | val_loss=0.4396\n",
      "Early stopping.\n",
      "\n",
      "DL (PyTorch) — Test ROC-AUC: 0.7453\n",
      "DL (PyTorch) — Test F1     : 0.2960\n",
      "\n",
      "DL Classification report (thr=0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8245    0.9573    0.8860     28199\n",
      "           1     0.5480    0.2028    0.2960      7205\n",
      "\n",
      "    accuracy                         0.8037     35404\n",
      "   macro avg     0.6863    0.5800    0.5910     35404\n",
      "weighted avg     0.7683    0.8037    0.7659     35404\n",
      "\n",
      "DL Confusion matrix:\n",
      "[[26994  1205]\n",
      " [ 5744  1461]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamya\\AppData\\Local\\Temp\\ipykernel_18920\\1903820758.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 4: PyTorch MLP — Train & Evaluate (AUC / F1) ====\n",
    "import os, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "INPUT_DIM  = X_train_np.shape[1]\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS     = 30\n",
    "LR         = 1e-3\n",
    "VAL_SPLIT  = 0.15\n",
    "MODEL_DIR  = \"model_mlp_default_risk\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"pytorch_mlp.pt\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.ReLU(), nn.Dropout(0.25),\n",
    "            nn.Linear(256, 128),    nn.ReLU(), nn.Dropout(0.25),\n",
    "            nn.Linear(128, 64),     nn.ReLU(), nn.Dropout(0.25),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):  # returns logits\n",
    "        return self.net(x)\n",
    "\n",
    "# Build dataset & split\n",
    "X_t = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_t = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32)\n",
    "ds  = TensorDataset(X_t, y_t)\n",
    "val_sz   = int(len(ds)*VAL_SPLIT)\n",
    "train_sz = len(ds)-val_sz\n",
    "train_ds, val_ds = random_split(ds, [train_sz, val_sz], generator=torch.Generator().manual_seed(42))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLP(INPUT_DIM).to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "bce   = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_auc, patience, wait = -1.0, 5, 0\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "def eval_auc(dl):\n",
    "    model.eval()\n",
    "    all_logits, all_y, total = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss   = bce(logits, yb)\n",
    "            total += loss.item()*len(xb)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_y.append(yb.cpu())\n",
    "    logits = torch.cat(all_logits).numpy().ravel()\n",
    "    ytrue  = torch.cat(all_y).numpy().ravel().astype(int)\n",
    "    yprob  = 1/(1+np.exp(-logits))\n",
    "    return roc_auc_score(ytrue, yprob), total/len(dl.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        loss = bce(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    val_auc, val_loss = eval_auc(val_dl)\n",
    "    print(f\"Epoch {ep:02d} | val_auc={val_auc:.4f} | val_loss={val_loss:.4f}\")\n",
    "    if val_auc > best_auc + 1e-4:\n",
    "        best_auc, wait = val_auc, 0\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Load best and evaluate on TEST\n",
    "best = MLP(INPUT_DIM).to(DEVICE)\n",
    "best.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "best.eval()\n",
    "with torch.no_grad():\n",
    "    logits_test = best(torch.tensor(X_test_np, dtype=torch.float32).to(DEVICE)).cpu().numpy().ravel()\n",
    "y_proba_dl = 1/(1+np.exp(-logits_test))\n",
    "y_pred_dl  = (y_proba_dl >= 0.5).astype(int)\n",
    "\n",
    "auc_dl = roc_auc_score(y_test, y_proba_dl)\n",
    "f1_dl  = f1_score(y_test, y_pred_dl)\n",
    "print(f\"\\nDL (PyTorch) — Test ROC-AUC: {auc_dl:.4f}\")\n",
    "print(f\"DL (PyTorch) — Test F1     : {f1_dl:.4f}\")\n",
    "print(\"\\nDL Classification report (thr=0.5):\")\n",
    "print(classification_report(y_test, y_pred_dl, digits=4))\n",
    "print(\"DL Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60afae44-bced-4a6c-a8d8-8002b16e8b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:34.00 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('float32')], shape=[(88,)]) reward_signature=Signature(dtype=[dtype('float32')], shape=[(1,)])\n",
      "2025-10-29 17:34.00 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2025-10-29 17:34.02 [info     ] Action size has been automatically determined. action_size=2\n",
      "[trainer] Starting CQL training for 100,000 steps (positional API).\n",
      "2025-10-29 17:34.02 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(88,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=2)\n",
      "2025-10-29 17:34.02 [info     ] Directory is created at d3rlpy_logs\\DiscreteCQL_20251029173402\n",
      "2025-10-29 17:34.02 [debug    ] Building models...            \n",
      "2025-10-29 17:34.02 [debug    ] Models have been built.       \n",
      "2025-10-29 17:34.02 [info     ] Parameters                     params={'observation_shape': [88], 'action_size': 2, 'config': {'type': 'discrete_cql', 'params': {'batch_size': 32, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'learning_rate': 6.25e-05, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 8000, 'alpha': 1.0}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████| 10000/10000 [01:35<00:00, 104.30it/s, loss=2.28e+3, td_loss=2.28e+3, conservative_loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:35.38 [info     ] DiscreteCQL_20251029173402: epoch=1 step=10000 epoch=1 metrics={'time_sample_batch': 0.001950192952156067, 'time_algorithm_update': 0.007246652841567993, 'loss': 2284.6669262908936, 'td_loss': 2283.6405488830565, 'conservative_loss': 1.0263784131526947, 'time_step': 0.009455177330970763} step=10000\n",
      "2025-10-29 17:35.38 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_10000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████████| 10000/10000 [01:42<00:00, 97.23it/s, loss=2.27e+3, td_loss=2.27e+3, conservative_loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:37.21 [info     ] DiscreteCQL_20251029173402: epoch=2 step=20000 epoch=2 metrics={'time_sample_batch': 0.0017347086429595948, 'time_algorithm_update': 0.008112270545959473, 'loss': 2275.357523045349, 'td_loss': 2274.293320652771, 'conservative_loss': 1.0642012583494187, 'time_step': 0.01013002429008484} step=20000\n",
      "2025-10-29 17:37.21 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_20000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█████████| 10000/10000 [01:44<00:00, 95.34it/s, loss=2.29e+3, td_loss=2.29e+3, conservative_loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:39.06 [info     ] DiscreteCQL_20251029173402: epoch=3 step=30000 epoch=3 metrics={'time_sample_batch': 0.001788645362854004, 'time_algorithm_update': 0.00826271939277649, 'loss': 2286.6103536483765, 'td_loss': 2285.5583837646486, 'conservative_loss': 1.0519686249405145, 'time_step': 0.010330669593811036} step=30000\n",
      "2025-10-29 17:39.06 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_30000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█████████| 10000/10000 [01:41<00:00, 98.77it/s, loss=2.27e+3, td_loss=2.27e+3, conservative_loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:40.47 [info     ] DiscreteCQL_20251029173402: epoch=4 step=40000 epoch=4 metrics={'time_sample_batch': 0.0017792885065078736, 'time_algorithm_update': 0.007895888662338257, 'loss': 2273.507225967407, 'td_loss': 2272.443395089722, 'conservative_loss': 1.0638314249038696, 'time_step': 0.009961041808128356} step=40000\n",
      "2025-10-29 17:40.47 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_40000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█████████| 10000/10000 [01:47<00:00, 92.59it/s, loss=2.28e+3, td_loss=2.28e+3, conservative_loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:42.35 [info     ] DiscreteCQL_20251029173402: epoch=5 step=50000 epoch=5 metrics={'time_sample_batch': 0.001859654927253723, 'time_algorithm_update': 0.008246270608901977, 'loss': 2278.993591516113, 'td_loss': 2277.921621609497, 'conservative_loss': 1.0719685282200575, 'time_step': 0.010522920179367066} step=50000\n",
      "2025-10-29 17:42.35 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_50000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█████████| 10000/10000 [01:51<00:00, 89.99it/s, loss=2.27e+3, td_loss=2.27e+3, conservative_loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:44.26 [info     ] DiscreteCQL_20251029173402: epoch=6 step=60000 epoch=6 metrics={'time_sample_batch': 0.001890283966064453, 'time_algorithm_update': 0.008564613676071166, 'loss': 2273.804710974121, 'td_loss': 2272.7331754089355, 'conservative_loss': 1.0715356320679188, 'time_step': 0.010842738986015319} step=60000\n",
      "2025-10-29 17:44.26 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_60000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|█████████| 10000/10000 [01:49<00:00, 91.08it/s, loss=2.27e+3, td_loss=2.27e+3, conservative_loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:46.16 [info     ] DiscreteCQL_20251029173402: epoch=7 step=70000 epoch=7 metrics={'time_sample_batch': 0.001917043948173523, 'time_algorithm_update': 0.008452614426612853, 'loss': 2269.1253551116943, 'td_loss': 2268.0493310821535, 'conservative_loss': 1.0760244467437268, 'time_step': 0.010736567091941833} step=70000\n",
      "2025-10-29 17:46.16 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_70000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|█████████| 10000/10000 [01:44<00:00, 95.55it/s, loss=2.28e+3, td_loss=2.28e+3, conservative_loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:48.01 [info     ] DiscreteCQL_20251029173402: epoch=8 step=80000 epoch=8 metrics={'time_sample_batch': 0.0018487441778182983, 'time_algorithm_update': 0.008175725674629212, 'loss': 2282.2005809188845, 'td_loss': 2281.119483039856, 'conservative_loss': 1.081098737603426, 'time_step': 0.010303324151039123} step=80000\n",
      "2025-10-29 17:48.01 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_80000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|█████████| 10000/10000 [01:43<00:00, 96.60it/s, loss=2.27e+3, td_loss=2.27e+3, conservative_loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:49.45 [info     ] DiscreteCQL_20251029173402: epoch=9 step=90000 epoch=9 metrics={'time_sample_batch': 0.0018027489900588989, 'time_algorithm_update': 0.008112362003326415, 'loss': 2266.9785000457764, 'td_loss': 2265.9045291030884, 'conservative_loss': 1.0739714259356261, 'time_step': 0.01019034698009491} step=90000\n",
      "2025-10-29 17:49.45 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_90000.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10000/10000 [01:42<00:00, 97.57it/s, loss=2.3e+3, td_loss=2.3e+3, conservative_loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:51.27 [info     ] DiscreteCQL_20251029173402: epoch=10 step=100000 epoch=10 metrics={'time_sample_batch': 0.0018259360551834106, 'time_algorithm_update': 0.007992677760124207, 'loss': 2296.7082074401856, 'td_loss': 2295.631825073242, 'conservative_loss': 1.0763824317485093, 'time_step': 0.01009698393344879} step=100000\n",
      "2025-10-29 17:51.27 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20251029173402\\model_100000.d3\n",
      "[trainer] Training complete.\n",
      "\n",
      "RL — Estimated Policy Value (empirical test): -1,248.34 per application\n",
      "RL — Approval rate (test): 92.79%\n",
      "RL — Approved & Fully Paid: 27030\n",
      "RL — Approved & Defaulted : 5821\n",
      "[Info] FQE unavailable/failed; skipping. Reason: _FQEBase.__init__() missing 1 required positional argument: 'config'\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 5: Offline RL — Build rewards, train CQL (robust), compute EPV ====\n",
    "import numpy as np, pandas as pd, d3rlpy\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "\n",
    "# Business reward function\n",
    "def compute_reward(approve: int, default_flag: int, loan_amnt: float, int_rate: float) -> float:\n",
    "    if approve == 0:\n",
    "        return 0.0\n",
    "    return (loan_amnt * (int_rate/100.0)) if default_flag == 0 else -float(loan_amnt)\n",
    "\n",
    "# Check required reward columns exist in your clean file\n",
    "assert \"loan_amnt\" in X.columns and \"int_rate\" in X.columns, \\\n",
    "    \"Your clean dataset must include 'loan_amnt' and 'int_rate' columns for reward calculation.\"\n",
    "\n",
    "# Build one-step transitions for BOTH actions per train sample\n",
    "def build_bandit_mdp(X_df, X_np, y):\n",
    "    obs, act, rew, ter = [], [], [], []\n",
    "    for i in range(len(y)):\n",
    "        s  = X_np[i]\n",
    "        la = float(X_df.iloc[i][\"loan_amnt\"])\n",
    "        ir = float(X_df.iloc[i][\"int_rate\"])\n",
    "        d  = int(y[i])\n",
    "        # deny (a=0)\n",
    "        obs.append(s); act.append(0); rew.append(compute_reward(0, d, la, ir)); ter.append(1.0)\n",
    "        # approve (a=1)\n",
    "        obs.append(s); act.append(1); rew.append(compute_reward(1, d, la, ir)); ter.append(1.0)\n",
    "    return (np.asarray(obs, np.float32),\n",
    "            np.asarray(act, np.int64),\n",
    "            np.asarray(rew, np.float32),\n",
    "            np.asarray(ter, np.float32))\n",
    "\n",
    "obs_tr, act_tr, rew_tr, ter_tr = build_bandit_mdp(X_train, X_train_np, y_train)\n",
    "train_dataset = MDPDataset(observations=obs_tr, actions=act_tr, rewards=rew_tr, terminals=ter_tr)\n",
    "\n",
    "# Construct a CQL agent in a version-robust way\n",
    "algo = None\n",
    "try:\n",
    "    from d3rlpy.algos import DiscreteCQL\n",
    "    try:\n",
    "        algo = DiscreteCQL(learning_rate=3e-4, batch_size=2048, n_steps=1, use_gpu=False)\n",
    "    except TypeError:\n",
    "        algo = None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if algo is None:\n",
    "    try:\n",
    "        from d3rlpy.algos import DiscreteCQLConfig\n",
    "        cfg = DiscreteCQLConfig()\n",
    "        if hasattr(cfg, \"create\"): algo = cfg.create(device=\"cpu\")\n",
    "        else:\n",
    "            from d3rlpy.algos import DiscreteCQL\n",
    "            algo = DiscreteCQL(cfg, device=\"cpu\")\n",
    "    except Exception:\n",
    "        from d3rlpy.algos import CQL, CQLConfig\n",
    "        cfg = CQLConfig()\n",
    "        algo = cfg.create(device=\"cpu\") if hasattr(cfg, \"create\") else CQL(cfg, device=\"cpu\")\n",
    "\n",
    "# Fit with a robust wrapper that tries multiple signatures\n",
    "# ---- Minimal trainer for older d3rlpy (positional n_steps only) ----\n",
    "# Many 1.x builds require: fit(dataset, n_steps) with no keywords.\n",
    "N_STEPS = 100_000  # you can increase later (e.g., 200k–500k)\n",
    "\n",
    "print(f\"[trainer] Starting CQL training for {N_STEPS:,} steps (positional API).\")\n",
    "algo.fit(train_dataset, N_STEPS)  # <-- positional n_steps only\n",
    "print(\"[trainer] Training complete.\")\n",
    "\n",
    "\n",
    "# Greedy action dispatcher across versions\n",
    "def greedy_action(algo, states: np.ndarray) -> np.ndarray:\n",
    "    for fn_name in [\"predict\", \"predict_best_action\", \"predict_action\"]:\n",
    "        if hasattr(algo, fn_name):\n",
    "            out = getattr(algo, fn_name)(states)\n",
    "            if isinstance(out, tuple): out = out[0]\n",
    "            return np.asarray(out, dtype=int)\n",
    "    raise RuntimeError(\"No compatible predict method found for greedy action.\")\n",
    "\n",
    "# Evaluate RL on test: empirical Estimated Policy Value (EPV)\n",
    "a_hat_rl = greedy_action(algo, X_test_np).astype(int)\n",
    "\n",
    "def row_reward(i, a):\n",
    "    la = float(X_test.iloc[i][\"loan_amnt\"])\n",
    "    ir = float(X_test.iloc[i][\"int_rate\"])\n",
    "    d  = int(y_test[i])\n",
    "    return compute_reward(a, d, la, ir)\n",
    "\n",
    "rewards_rl    = np.array([row_reward(i, a_hat_rl[i]) for i in range(len(y_test))], dtype=float)\n",
    "epv_empirical = rewards_rl.mean()\n",
    "approval_rate = float((a_hat_rl == 1).mean())\n",
    "approved_paid = int(((a_hat_rl == 1) & (y_test == 0)).sum())\n",
    "approved_def  = int(((a_hat_rl == 1) & (y_test == 1)).sum())\n",
    "\n",
    "print(f\"\\nRL — Estimated Policy Value (empirical test): {epv_empirical:,.2f} per application\")\n",
    "print(f\"RL — Approval rate (test): {approval_rate*100:.2f}%\")\n",
    "print(f\"RL — Approved & Fully Paid: {approved_paid}\")\n",
    "print(f\"RL — Approved & Defaulted : {approved_def}\")\n",
    "\n",
    "# Optional: FQE if available\n",
    "try:\n",
    "    from d3rlpy.ope import FQE\n",
    "    fqe = FQE(algo)\n",
    "    fqe.fit(train_dataset, n_epochs=5, verbose=False)\n",
    "    v_est = float(fqe.predict_value(X_test_np).mean())\n",
    "    print(f\"RL — FQE Estimated Policy Value (approx): {v_est:,.2f} per application\")\n",
    "except Exception as e:\n",
    "    v_est = None\n",
    "    print(\"[Info] FQE unavailable/failed; skipping. Reason:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7d1702c-b664-4fec-bbeb-b88bf79312b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total disagreements: 697\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>DL_policy</th>\n",
       "      <th>RL_policy</th>\n",
       "      <th>True_Default</th>\n",
       "      <th>DL_PD</th>\n",
       "      <th>RL_reward_if_taken</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Deny</td>\n",
       "      <td>1</td>\n",
       "      <td>0.472188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19075.0</td>\n",
       "      <td>17.57</td>\n",
       "      <td>53000.0</td>\n",
       "      <td>22.12</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>debt_consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>Deny</td>\n",
       "      <td>Approve</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501262</td>\n",
       "      <td>-20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>18.25</td>\n",
       "      <td>62500.0</td>\n",
       "      <td>19.49</td>\n",
       "      <td>10.0</td>\n",
       "      <td>77.1</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Deny</td>\n",
       "      <td>0</td>\n",
       "      <td>0.480644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20500.0</td>\n",
       "      <td>19.19</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>32.19</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.7</td>\n",
       "      <td>debt_consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Deny</td>\n",
       "      <td>0</td>\n",
       "      <td>0.477234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>18.25</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>18.24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.7</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Deny</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15.61</td>\n",
       "      <td>100300.0</td>\n",
       "      <td>21.09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>67.1</td>\n",
       "      <td>credit_card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>330</td>\n",
       "      <td>Deny</td>\n",
       "      <td>Approve</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548874</td>\n",
       "      <td>-20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>13.67</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>14.70</td>\n",
       "      <td>10.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>390</td>\n",
       "      <td>Deny</td>\n",
       "      <td>Approve</td>\n",
       "      <td>1</td>\n",
       "      <td>0.525580</td>\n",
       "      <td>-15300.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>18.55</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>17.72</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>428</td>\n",
       "      <td>Deny</td>\n",
       "      <td>Approve</td>\n",
       "      <td>0</td>\n",
       "      <td>0.504016</td>\n",
       "      <td>4797.6</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>19.99</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>21.45</td>\n",
       "      <td>10.0</td>\n",
       "      <td>51.8</td>\n",
       "      <td>debt_consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>519</td>\n",
       "      <td>Deny</td>\n",
       "      <td>Approve</td>\n",
       "      <td>1</td>\n",
       "      <td>0.520685</td>\n",
       "      <td>-33425.0</td>\n",
       "      <td>33425.0</td>\n",
       "      <td>14.65</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>14.30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>584</td>\n",
       "      <td>Deny</td>\n",
       "      <td>Approve</td>\n",
       "      <td>1</td>\n",
       "      <td>0.538829</td>\n",
       "      <td>-33100.0</td>\n",
       "      <td>33100.0</td>\n",
       "      <td>24.24</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>23.77</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.3</td>\n",
       "      <td>debt_consolidation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx DL_policy RL_policy  True_Default     DL_PD  RL_reward_if_taken  \\\n",
       "0   93   Approve      Deny             1  0.472188                 0.0   \n",
       "1   94      Deny   Approve             1  0.501262            -20000.0   \n",
       "2  112   Approve      Deny             0  0.480644                 0.0   \n",
       "3  195   Approve      Deny             0  0.477234                 0.0   \n",
       "4  224   Approve      Deny             1  0.475880                 0.0   \n",
       "5  330      Deny   Approve             1  0.548874            -20000.0   \n",
       "6  390      Deny   Approve             1  0.525580            -15300.0   \n",
       "7  428      Deny   Approve             0  0.504016              4797.6   \n",
       "8  519      Deny   Approve             1  0.520685            -33425.0   \n",
       "9  584      Deny   Approve             1  0.538829            -33100.0   \n",
       "\n",
       "   loan_amnt  int_rate  annual_inc    dti  emp_length  revol_util  \\\n",
       "0    19075.0     17.57     53000.0  22.12         9.0        28.6   \n",
       "1    20000.0     18.25     62500.0  19.49        10.0        77.1   \n",
       "2    20500.0     19.19     57000.0  32.19         0.5        29.7   \n",
       "3    21000.0     18.25     80000.0  18.24         3.0        48.7   \n",
       "4    15000.0     15.61    100300.0  21.09         4.0        67.1   \n",
       "5    20000.0     13.67     72000.0  14.70        10.0        72.0   \n",
       "6    15300.0     18.55     38000.0  17.72        10.0        50.0   \n",
       "7    24000.0     19.99     70000.0  21.45        10.0        51.8   \n",
       "8    33425.0     14.65    200000.0  14.30         6.0         8.1   \n",
       "9    33100.0     24.24     72000.0  23.77        10.0        64.3   \n",
       "\n",
       "              purpose  \n",
       "0  debt_consolidation  \n",
       "1               other  \n",
       "2  debt_consolidation  \n",
       "3               other  \n",
       "4         credit_card  \n",
       "5  debt_consolidation  \n",
       "6             medical  \n",
       "7  debt_consolidation  \n",
       "8  debt_consolidation  \n",
       "9  debt_consolidation  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Cell 6: Policy comparison — find disagreement examples ====\n",
    "import pandas as pd\n",
    "# DL policy: approve if predicted default prob < 0.5\n",
    "dl_approve = (y_proba_dl < 0.5).astype(int)\n",
    "rl_approve = a_hat_rl\n",
    "\n",
    "disagree_idx = np.where(dl_approve != rl_approve)[0]\n",
    "print(f\"Total disagreements: {len(disagree_idx)}\")\n",
    "\n",
    "show_cols = [\"loan_amnt\",\"int_rate\",\"annual_inc\",\"dti\",\"emp_length\",\"revol_util\",\"purpose\"]\n",
    "cols_present = [c for c in show_cols if c in X_test.columns]\n",
    "\n",
    "def rl_row_reward(i):\n",
    "    return row_reward(i, rl_approve[i])\n",
    "\n",
    "rows = []\n",
    "for i in disagree_idx[:20]:  # display up to 20 examples\n",
    "    r = {\n",
    "        \"idx\": i,\n",
    "        \"DL_policy\": \"Approve\" if dl_approve[i]==1 else \"Deny\",\n",
    "        \"RL_policy\": \"Approve\" if rl_approve[i]==1 else \"Deny\",\n",
    "        \"True_Default\": int(y_test[i]),\n",
    "        \"DL_PD\": float(y_proba_dl[i]),\n",
    "        \"RL_reward_if_taken\": rl_row_reward(i),\n",
    "    }\n",
    "    for c in cols_present:\n",
    "        r[c] = X_test.iloc[i][c]\n",
    "    rows.append(r)\n",
    "\n",
    "ex_df = pd.DataFrame(rows)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(ex_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a17a1af1-afbe-4aad-8d2b-dc41b0c41959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL ANALYSIS — Default Risk Classifier (DL, PyTorch) vs Profit-Seeking Policy (Offline RL, CQL)\n",
      "\n",
      "1) Key Results on Held-Out Test Set\n",
      "   • Deep Learning (DL): ROC-AUC = 0.7453, F1@0.5 = 0.2960\n",
      "   • Offline RL (CQL): Estimated Policy Value (empirical) = -1,248.34 per application\n",
      "   • RL Approval rate: 92.79%; Approved Paid: 27030, Approved Defaulted: 5821\n",
      "\n",
      "2) Metric Rationale\n",
      "   • DL metrics — AUC captures ranking quality across thresholds; F1 summarizes precision/recall at the chosen operating point on imbalanced data.\n",
      "   • RL metric — Estimated Policy Value (EPV) is the average per-application reward under the learned policy (profit minus loss), aligning directly to business ROI.\n",
      "\n",
      "3) Policy Comparison & Insights\n",
      "   • Disagreements: 697 cases where DL and RL differ.\n",
      "   • RL can approve some DL-flagged ‘risky’ applicants when expected interest (loan_amnt × int_rate) outweighs learned default risk → positive expected profit.\n",
      "   • RL can deny some DL-approvals when upside is weak (low interest) or features signal higher loss risk → negative expected profit avoided.\n",
      "\n",
      "4) Future Steps (Practical Plan)\n",
      "   • Economic thresholding for DL: tune threshold to maximize expected value, not just F1; calibrate probabilities (Platt/Isotonic).\n",
      "   • Reward realism: include fees, LGD<1, recoveries, prepayments, funding/servicing costs, and time value (NPV).\n",
      "   • Portfolio constraints & fairness: min/max approval by segment, risk caps, bias audits; deploy with guardrails.\n",
      "   • OPE hardening: log behavior propensities; consider IPS/DR estimators; FQE with uncertainty bands.\n",
      "   • Deployment: ship calibrated DL as champion; run RL as challenger (shadow/A/B) with tight drift monitoring.\n",
      "   • Data wishlist: bureau aggregates, trend features, fraud/device signals, application channel, macro-at-issue, recovery details.\n",
      "   • Algorithms: XGBoost/LightGBM/CatBoost, TabNet, Wide&Deep; RL: IQL/BCQ, conservative model-based RL, constrained optimization for portfolio targets.\n",
      "\n",
      "5) Limitations\n",
      "   • One-step bandit framing ignores payment timing & partial recoveries; reward assumptions drive outcomes.\n",
      "   • Offline RL value depends on logged data coverage; distribution shift and rare outcomes can bias estimates.\n",
      "   • Empirical EPV assumes the test set well-represents deployment distribution; monitor and recalibrate regularly.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 7: Final report — auto-filled text you can paste ====\n",
    "import textwrap\n",
    "\n",
    "lines = []\n",
    "lines.append(\"FINAL ANALYSIS — Default Risk Classifier (DL, PyTorch) vs Profit-Seeking Policy (Offline RL, CQL)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"1) Key Results on Held-Out Test Set\")\n",
    "lines.append(f\"   • Deep Learning (DL): ROC-AUC = {auc_dl:.4f}, F1@0.5 = {f1_dl:.4f}\")\n",
    "lines.append(f\"   • Offline RL (CQL): Estimated Policy Value (empirical) = {epv_empirical:,.2f} per application\")\n",
    "if 'v_est' in locals() and v_est is not None:\n",
    "    lines.append(f\"                   FQE Estimated Policy Value (approx) = {v_est:,.2f} per application\")\n",
    "lines.append(f\"   • RL Approval rate: {approval_rate*100:.2f}%; Approved Paid: {approved_paid}, Approved Defaulted: {approved_def}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"2) Metric Rationale\")\n",
    "lines.append(\"   • DL metrics — AUC captures ranking quality across thresholds; F1 summarizes precision/recall at the chosen operating point on imbalanced data.\")\n",
    "lines.append(\"   • RL metric — Estimated Policy Value (EPV) is the average per-application reward under the learned policy (profit minus loss), aligning directly to business ROI.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"3) Policy Comparison & Insights\")\n",
    "lines.append(f\"   • Disagreements: {len(disagree_idx)} cases where DL and RL differ.\")\n",
    "lines.append(\"   • RL can approve some DL-flagged ‘risky’ applicants when expected interest (loan_amnt × int_rate) outweighs learned default risk → positive expected profit.\")\n",
    "lines.append(\"   • RL can deny some DL-approvals when upside is weak (low interest) or features signal higher loss risk → negative expected profit avoided.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"4) Future Steps (Practical Plan)\")\n",
    "lines.append(\"   • Economic thresholding for DL: tune threshold to maximize expected value, not just F1; calibrate probabilities (Platt/Isotonic).\")\n",
    "lines.append(\"   • Reward realism: include fees, LGD<1, recoveries, prepayments, funding/servicing costs, and time value (NPV).\")\n",
    "lines.append(\"   • Portfolio constraints & fairness: min/max approval by segment, risk caps, bias audits; deploy with guardrails.\")\n",
    "lines.append(\"   • OPE hardening: log behavior propensities; consider IPS/DR estimators; FQE with uncertainty bands.\")\n",
    "lines.append(\"   • Deployment: ship calibrated DL as champion; run RL as challenger (shadow/A/B) with tight drift monitoring.\")\n",
    "lines.append(\"   • Data wishlist: bureau aggregates, trend features, fraud/device signals, application channel, macro-at-issue, recovery details.\")\n",
    "lines.append(\"   • Algorithms: XGBoost/LightGBM/CatBoost, TabNet, Wide&Deep; RL: IQL/BCQ, conservative model-based RL, constrained optimization for portfolio targets.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"5) Limitations\")\n",
    "lines.append(\"   • One-step bandit framing ignores payment timing & partial recoveries; reward assumptions drive outcomes.\")\n",
    "lines.append(\"   • Offline RL value depends on logged data coverage; distribution shift and rare outcomes can bias estimates.\")\n",
    "lines.append(\"   • Empirical EPV assumes the test set well-represents deployment distribution; monitor and recalibrate regularly.\")\n",
    "print(textwrap.dedent(\"\\n\".join(lines)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a730992-d155-41cd-b35a-957326b8686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved RL policy to offline_rl_cql/cql_discrete_model.d3\n",
      "Saved preprocessor to offline_rl_cql/preprocess.joblib\n",
      "Saved DL (PyTorch) to model_mlp_default_risk/pytorch_mlp.pt\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 8 (optional): Save artifacts ====\n",
    "import joblib, d3rlpy, os\n",
    "\n",
    "# Save DL model (PyTorch)\n",
    "os.makedirs(\"model_mlp_default_risk\", exist_ok=True)\n",
    "torch.save(best.state_dict(), \"model_mlp_default_risk/pytorch_mlp.pt\")\n",
    "\n",
    "# Save preprocessor for serving (so you can transform raw inputs the same way)\n",
    "os.makedirs(\"offline_rl_cql\", exist_ok=True)\n",
    "joblib.dump(preprocess, \"offline_rl_cql/preprocess.joblib\")\n",
    "\n",
    "# Save RL policy\n",
    "try:\n",
    "    algo.save(\"offline_rl_cql/cql_discrete_model.d3\")\n",
    "    print(\"Saved RL policy to offline_rl_cql/cql_discrete_model.d3\")\n",
    "except Exception as e:\n",
    "    print(\"Could not save RL policy (ok to ignore for now):\", e)\n",
    "print(\"Saved preprocessor to offline_rl_cql/preprocess.joblib\")\n",
    "print(\"Saved DL (PyTorch) to model_mlp_default_risk/pytorch_mlp.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db11bb5-b6dd-444d-a7ba-6c5bcf6ebf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (rl-fintech)",
   "language": "python",
   "name": "rl-fintech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
